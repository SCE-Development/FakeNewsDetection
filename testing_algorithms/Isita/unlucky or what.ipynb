{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torchtext.data import LabelField ,Field, TabularDataset, BucketIterator, Iterator\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>donald trump sends embarrassing new year eve m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drunk bragging trump staffer started russian c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sheriff david clarke internet joke threatening...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trump obsessed obama coded website image chris...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pope francis called donald trump christmas spe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category\n",
       "0  donald trump sends embarrassing new year eve m...         1\n",
       "1  drunk bragging trump staffer started russian c...         1\n",
       "2  sheriff david clarke internet joke threatening...         1\n",
       "3  trump obsessed obama coded website image chris...         1\n",
       "4  pope francis called donald trump christmas spe...         1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/cleandata/cleaned.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3503</th>\n",
       "      <td>prominent holocaust attorney file massive laws...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43936</th>\n",
       "      <td>powerful hurricane fuel demand island nation c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29589</th>\n",
       "      <td>trump moving u era bilateral trade white house...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27989</th>\n",
       "      <td>democrat amass support force showdown trump su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30224</th>\n",
       "      <td>trump pack trade team veteran steel war chinaw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  category\n",
       "3503   prominent holocaust attorney file massive laws...         1\n",
       "43936  powerful hurricane fuel demand island nation c...         0\n",
       "29589  trump moving u era bilateral trade white house...         0\n",
       "27989  democrat amass support force showdown trump su...         0\n",
       "30224  trump pack trade team veteran steel war chinaw...         0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac = 1)#.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.10,\n",
    "    random_state=42,    \n",
    ")\n",
    "\n",
    "train, val = train_test_split(\n",
    "    train,\n",
    "    test_size=0.10,\n",
    "    random_state=42,    \n",
    ")\n",
    "\n",
    "train.to_csv(\"train.csv\", index=False)\n",
    "test.to_csv(\"test.csv\", index=False)\n",
    "val.to_csv(\"val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (516 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "here2\n",
      "here3\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print('here')\n",
    "# Model parameter\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "\n",
    "# Fields\n",
    "print('here2')\n",
    "category_field = LabelField(dtype=torch.float)\n",
    "text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
    "                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
    "fields = [('text', text_field), ('category', category_field)]\n",
    "\n",
    "# TabularDataset\n",
    "print('here3')\n",
    "train, valid, test = TabularDataset.splits(\n",
    "    path='/Users/admin/Desktop/moreML/FakeNewsDetection/testing_algorithms/Isita',\n",
    "    train='train.csv', validation='val.csv', test='test.csv', format='CSV', fields=fields, skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda2/envs/cs185c/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/opt/anaconda2/envs/cs185c/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: Iterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Iterators\n",
    "\n",
    "train_iter = BucketIterator(dataset = train, batch_size=16, sort_key=lambda x: len(x.text),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "valid_iter = BucketIterator(valid, batch_size=16, sort_key=lambda x: len(x.text),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "test_iter = Iterator(test, batch_size=16, device=device, train=False, shuffle=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'TabularDataset' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-9c4b5fd0e85f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'TabularDataset' object is not callable"
     ]
    }
   ],
   "source": [
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "\n",
    "        options_name = \"bert-base-uncased\"\n",
    "        self.encoder = BertForSequenceClassification.from_pretrained(options_name)\n",
    "\n",
    "    def forward(self, text, label):\n",
    "        loss, text_fea = self.encoder(text, labels=label)[:2]\n",
    "\n",
    "        return loss, text_fea\n",
    "    def save_checkpoint(save_path, model, valid_loss):\n",
    "\n",
    "        if save_path == None:\n",
    "            return\n",
    "\n",
    "        state_dict = {'model_state_dict': model.state_dict(),\n",
    "                      'valid_loss': valid_loss}\n",
    "\n",
    "        torch.save(state_dict, save_path)\n",
    "        print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "    def load_checkpoint(load_path, model):\n",
    "\n",
    "        if load_path==None:\n",
    "            return\n",
    "\n",
    "        state_dict = torch.load(load_path, map_location=device)\n",
    "        print(f'Model loaded from <== {load_path}')\n",
    "\n",
    "        model.load_state_dict(state_dict['model_state_dict'])\n",
    "        return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "    def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
    "        if(True):\n",
    "            print('i die')\n",
    "        if save_path == None:\n",
    "            return\n",
    "\n",
    "        state_dict = {'train_loss_list': train_loss_list,\n",
    "                      'valid_loss_list': valid_loss_list,\n",
    "                      'global_steps_list': global_steps_list}\n",
    "\n",
    "        torch.save(state_dict, save_path)\n",
    "        print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "    def load_metrics(load_path):\n",
    "\n",
    "        if load_path==None:\n",
    "            return\n",
    "\n",
    "        state_dict = torch.load(load_path, map_location=device)\n",
    "        print(f'Model loaded from <== {load_path}')\n",
    "\n",
    "        return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']\n",
    "    \n",
    "    def train(model,\n",
    "          optimizer,\n",
    "          criterion = nn.BCELoss(),\n",
    "          train_loader = train_iter,\n",
    "          valid_loader = valid_iter,\n",
    "          num_epochs = 5,\n",
    "          eval_every = len(train_iter) // 2,\n",
    "          best_valid_loss = float(\"Inf\")):\n",
    "    \n",
    "        # initialize running values\n",
    "        running_loss = 0.0\n",
    "        valid_running_loss = 0.0\n",
    "        global_step = 0\n",
    "        train_loss_list = []\n",
    "        valid_loss_list = []\n",
    "        global_steps_list = []\n",
    "\n",
    "        # training loop\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for (labels, text), _ in train_loader:\n",
    "                labels = labels.type(torch.LongTensor)           \n",
    "                labels = labels.to(device)\n",
    "                text = text.type(torch.LongTensor)  \n",
    "                text = text.to(device)\n",
    "                output = model(text, labels)\n",
    "                loss, _ = output\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # update running values\n",
    "                running_loss += loss.item()\n",
    "                global_step += 1\n",
    "\n",
    "                # evaluation step\n",
    "                if global_step % eval_every == 0:\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():                    \n",
    "\n",
    "                        # validation loop\n",
    "                        for (labels, text), _ in valid_loader:\n",
    "                            labels = labels.type(torch.LongTensor)           \n",
    "                            labels = labels.to(device)\n",
    "                            text = text.type(torch.LongTensor)  \n",
    "                            text = text.to(device)\n",
    "                            output = model(text, labels)\n",
    "                            loss, _ = output\n",
    "\n",
    "                            valid_running_loss += loss.item()\n",
    "\n",
    "                    # evaluation\n",
    "                    average_train_loss = running_loss / eval_every\n",
    "                    average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "                    train_loss_list.append(average_train_loss)\n",
    "                    valid_loss_list.append(average_valid_loss)\n",
    "                    global_steps_list.append(global_step)\n",
    "\n",
    "                    # resetting running values\n",
    "                    running_loss = 0.0                \n",
    "                    valid_running_loss = 0.0\n",
    "                    model.train()\n",
    "\n",
    "                    # print progress\n",
    "                    print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                          .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "                                  average_train_loss, average_valid_loss))\n",
    "\n",
    "                    # checkpoint\n",
    "#                     if best_valid_loss > average_valid_loss:\n",
    "#                         best_valid_loss = average_valid_loss\n",
    "#                         save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n",
    "#                         save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "\n",
    "#         save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "        print('Finished Training!')\n",
    "\n",
    "model = BERT().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "train(model=model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs185c",
   "language": "python",
   "name": "cs185c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
